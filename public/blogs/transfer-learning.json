{
    "title": "Transfer Learning and Fine Tuning - Composable AI Protocol",
    "author": "Xin Jin",
    "createDate": "2024-01-21",
    "content": "_Transfer learning and fine-tuning are two techniques to achieve composability with pre-trained models._\n\n\n![alt_text](/assets/blog-images/transfer-learning.png \"Transfer learning overview\")\n\n\nTransfer learning enables efficient knowledge transfer (source: [link](https://neptune.ai/blog/transfer-learning-guide-examples-for-images-and-text-in-keras))\n\n**Transfer Learning**\n\nTransfer learning applies pre-trained models as the inputs and starts for a new task or domain. The idea is to train pre-trained models on large datasets and then retrain such models on smaller datasets for different tasks. This will save time and computational resources as the newly added model layers are minimal.\n\nThe pre-trained model is the feature extractor with frozen model weights and parameters. The weights of the newly added neural network layers are trained on much smaller datasets or task-specific datasets. Fine-tuning allows some pre-trained model layers to be adapted and learned, tailored to the new tasks and domains.\n\nA great example is Google's open-source transformer pre-trained models, which are used by startups and companies to build various applications - chatbot, named entity tagging, feed recommendations, and ad ranking.\n\nFor more information, please refer to the following [engineering blog](https://encord.com/blog/transfer-learning/). There are a lot of related machine learning techniques, like [mixture of experts](https://huggingface.co/blog/moe) to reduce the cost for pre-trained models and [domain adaptation](https://valeoai.github.io/blog/projects/domain-adaptation) to adapt to conditions that differ from those met during training.\n\n**What is next?**\n\nMainstream AI applications and models are composable on Nimble. \n\nThis is the 4th blog of the case study series. To learn more, please refer to blogs on the rest of the case studies. "
  }
