{
    "title": "Data Composability in AI",
    "author": "Xin Jin",
    "createDate": "2024-01-06",
    "content": "_Data in AI systems are nodes on the composable AI graph, while the edges are data transformations._\n\n\n![alt_text](https://nimble-homepage-blogs.s3.ap-southeast-1.amazonaws.com/assets/blog-images/computation-composability.png \"Data composability overview\")\n\n\nIn Nimble, miners achieve data composability for AI modeling and computations. For flexibility and reusability, this is achieved with modular analytics.\n\nThere are three types of data flow in any AI system. They are composable pieces that can be reused by all AI model miners.\n\n**Model Features**\n\nModel features are the inputs of AI models. They are the foundation of model training and inference. Each feature is a measurable data piece or variable on which AI makes decisions. For example, the TikTok recommendation model learns which video you like based on features including but not limited to your previously viewed video categories, location, age classes, and so on.\n\nModel features are composable since the same feature can be reused for multiple AI tasks. TikTok can use your age for video click predictions, ads ranking, search suggestions, and influencer recommendations. AI models can experiment with different model feature combinations without worrying about building the same set of features across multiple models.\n\nIn the era of deep learning and large AI models, features are mostly captured as vector embeddings. Each embedding is a vector of numbers that capture meanings and relationships. As shown below, embeddings with similar meanings or closer relationships are clustered when we apply embedding clustering algorithms.\n\n\n![alt_text](https://nimble-homepage-blogs.s3.ap-southeast-1.amazonaws.com/assets/blog-images/data-composability.png \"Data composability in AI\")\n\n\n**Model Predictions**\n\nModel predictions are the decisions made by AI. They can be the predictions of tree-based models, neural networks, and large AI models (e.g., large language models). The predictions live in either batch storage systems (e.g., S3, HDFS, and Hive) or real-time databases (e.g., MongoDB, MySQL, and Cassandra), together with model features.\n\nAI predictions are reusable, as other AI systems can use them as the inputs of AI models and further feature computations. Another model may reuse them directly for AI predictions. They may not be ready directly for some AI tasks and require more refined data processing. Such further data processing generates features used by more AI tasks.\n\nModel predictions are also modular by piecing them together for something greater. A task can be too complex to complete by one AI model. In the Uber fraud detection system, there are numerous AI models running for account integrity checks, trip level healthiness scoring and so on. All such AI models orchestrate together to determine whether a trip is fraudulent. Even if such models are initially built for fraud detection, they can be reused for user growth, trip pricing, and promotion calculations.\n\n**Model Files**\n\nModel files are a set of parameters to represent all the information about models. Their size is as small as one binary variable or as big as terabytes. Pretraining and fine-tuning are the two most popular model composability techniques.\n\nThe sizes of large AI models are all at least hundreds of gigabytes. Training such models is costly (e.g., tens of millions to train a large GPT model from scratch). Reusability and composability are thus critical to the model cost reduction. Pretraining is a technique in which there is a foundation model. The foundation model is a big model trained on large-scale datasets but not being re-trained frequently or is prohibitively expensive for smaller organizations.\n\nThe model files are composable since they represent all the information about a model. Bigger organizations open source the foundation models as parameter files. Individuals and smaller organizations fine-tune such models on smaller datasets for various tasks at a minimal cost. In the early days, OpenAI fine-tuned ChatGPT on top of the transformer models released publicly by Google. Meta has also released Open and Efficient Foundation Language Models (LLaMA).\n\n**What is next?**\n\nNimble is a composable AI protocol with such data composability for permissionless innovations. Model features, predictions and files are composable pieces, reusable for any protocol AI tasks.\n\nThis is the first blog of the AI composability series. To learn more, please refer to blogs on model and computation composability."
  }
