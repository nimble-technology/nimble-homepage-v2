{
    "title": "Model Composability in AI",
    "author": "Xin Jin",
    "createDate": "2024-01-16",
    "content": "_Models in AI systems are a type of data transformation on the composable AI graph, while the nodes are data._\n\n\n![alt_text](/assets/blog-images/computation-composability.png \"Model composability overview\")\n\n\nIn Nimble, miners achieve model composability for both AI training and inferences. For flexibility and reusability this is achieved with modular AI model building. In the data composability blog, foundation models and fine-tuning are discussed briefly for model file composability. Model composability defines why model file composability is possible.\n\nAI models transform data. Feature extractions are another type of data transformation, which runs similarly to AI models for composability as pipelines and services. The feature development process is entangled with AI training. For simplicity, we ignore its discussions in this blog series.\n\nThere are the following two types of model flows in any AI system. They are composable pieces that can be reused by all AI model miners.\n\n**AI Training**\n\nAI Training is the process of learning model parameter files by iterating model parameters given the training dataset. The training data guides the model iterations. The model itself is a set of model logic. The architecture defines the model structure in tree-based models by organizing the tree structures (e.g., random forest and gradient-boosting trees). In neural network models, it is how neurons are connected.\n\nModel training is a complex process: model development, optimization, and evaluation. Model development involves architecture design, feature engineering, and loss function selections. It is a complex and manual process, even for top AI engineers and researchers. The model optimization involves hyperparameter tuning and optimizations and training dataset iterations. The model evaluation consists of offline validation on the validation dataset and online evaluation in real-time systems with real user data and traffic.\n\nModern model file standards and development frameworks make it possible to make AI training composable. In this way, AI training code and outputs are reusable for different AI tasks.\n\n**AI Predictions**\n\nMultiple AI models collaborate for complex AI tasks to achieve the same goal. Different AI predictions are run in batch and online systems. They run as independent prediction pipelines and services. Such AI prediction units work together as reusable components for numerous tasks.\n\nPipeline and microservice frameworks enable such composability. For example, [Airflow](https://airflow.apache.org/) is the most adopted pipeline framework, while [gRPC](https://grpc.io/) and [Restful APIs](https://www.redhat.com/en/topics/api/what-is-a-rest-api) simplify online model prediction service integrations. They run as composable model components by calling each other independent service providers. The model predictions are persisted in the data stores, forming reusable data components as discussed in the data composability blog.\n\n**What is next?**\n\nNimble is a composable AI protocol with such model composability for permissionless innovations. AI training and predictions are composable pieces, reusable for any AI tasks in the protocol.\n\nThis is the 2nd blog of the AI composability series. To learn more, please refer to blogs on data and computation composability."
  }
