{
    "title": "Decentralizing Large Language Models",
    "author": "Xin Jin",
    "createDate": "2024-01-20",
    "content": "_Large language models are composable on Nimble._\n\n\n![alt_text](https://nimble-homepage-blogs.s3.ap-southeast-1.amazonaws.com/assets/blog-images/large-language-models.png \"Large language models overview\")\n\n\n**Reinforcement Learning from Human Feedback**\n\nReinforcement Learning from Human Feedback (RLHF) is a large language model (LLM) technique to guide language model training with human text prompt inputs. A reward model is trained for human inputs with further reinforcement learning (RL) fine-tuning techniques. For more information on the model details, [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf).\n\nAt a high level, the model is composable with a foundation model, a reward model for human feedback, and RL fine-tuning. There are numerous open-source foundation model versions. Reward model and RL fine tuning can be performed like user models in ranking systems. Thus, we focus the discussions on human feedback composability.\n\n**Composable Human Feedbacks**\n\nDifferent LLMs require various human text inputs for model performance improvements. It is both an automated and manual process. Researchers and engineers can easily modularize the automation part, as discussed in previous blogs. In this blog, we focus the discussions on manual human feedback composability.\n\nIn companies like OpenAI, manual human feedback is achieved with human labels. Such workers previously worked for large organizations and private companies like Google, Microsoft, Meta, and OpenAI. In our composable AI protocol, they contribute human text prompts and human scoring as independent miners. Reward models for different LLM purposes reward them with our network tokens. Model components like reward models and RL fine-tuning models receive network token rewards from the final model evaluation system, depending on their model performance improvements.\n\nIn LLM systems, both the model itself and human feedback are composable on Nimble.\n\n**What is next?**\n\nMainstream AI applications and models are composable on Nimble. \n\nThis is the 1st blog of the case study series. To learn more, please refer to blogs on the ranking system and other case studies. "
  }
