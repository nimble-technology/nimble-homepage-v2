{
    "title": "Multi-Task Learning in Deep Neural Networks - Composable AI Protocol",
    "author": "Xin Jin",
    "createDate": "2024-01-20",
    "content": "_Multi-task learning is an AI technique to reuse the same model components for different application tasks._\n\n\n![alt_text](https://nimble-homepage-blogs.s3.ap-southeast-1.amazonaws.com/assets/blog-images/multi-task-learning.png \"Multitask learning overview\")\n\n\nFrom separate single-model training to multi-task learning (source: [link](https://engineering.linkedin.com/blog/2022/applying-multitask-learning-to-ai-models-at-linkedin))\n\n**Multi-task Learning**\n\nMultitask learning, a subfield of machine learning, aims to accomplish multiple learning tasks simultaneously by exploiting commonalities and differences across tasks. Traditionally, engineers have built separate AI models to accomplish each task. While this allows a simple ownership boundary, the isolated models cannot share any learnings gathered from training the models. For example, new machine learning (ML) tasks often suffer from the cold start problem due to a lack of training data.\n\nFor more information on multi-task learning, please refer to the [engineering blog](https://engineering.linkedin.com/blog/2022/applying-multitask-learning-to-ai-models-at-linkedin).\n\n**Shared Model**\n\nThe shared model has the same model architecture reused by different tasks. Specific tasks optimize their model architectures with different input layers or loss functions. A great example is that ads ranking and feed recommendation tasks can use the same user profiling model in companies like Tiktok and Meta. The same shared model becomes a composable block in the centralized AI platforms owned by such companies.\n\n**What is next?**\n\nMainstream AI applications and models are composable on Nimble. \n\nThis is the 3rd blog of the case study series. Please refer to blogs on the transfer learning and other case studies to learn more."
  }
